config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]config.json: 100%|██████████| 665/665 [00:00<00:00, 5.33MB/s]
vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.61MB/s]vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.60MB/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.90MB/s]merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.88MB/s]
tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.30MB/s]tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.27MB/s]
model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]model.safetensors:   2%|▏         | 10.5M/548M [00:00<00:40, 13.4MB/s]model.safetensors:   4%|▍         | 21.0M/548M [00:01<00:25, 21.1MB/s]model.safetensors:   6%|▌         | 31.5M/548M [00:01<00:19, 25.9MB/s]model.safetensors:   8%|▊         | 41.9M/548M [00:01<00:17, 29.2MB/s]model.safetensors:  10%|▉         | 52.4M/548M [00:01<00:15, 31.4MB/s]model.safetensors:  11%|█▏        | 62.9M/548M [00:02<00:14, 32.9MB/s]model.safetensors:  13%|█▎        | 73.4M/548M [00:02<00:13, 34.0MB/s]model.safetensors:  15%|█▌        | 83.9M/548M [00:02<00:13, 34.7MB/s]model.safetensors:  17%|█▋        | 94.4M/548M [00:03<00:12, 35.1MB/s]model.safetensors:  19%|█▉        | 105M/548M [00:03<00:12, 35.5MB/s] model.safetensors:  21%|██        | 115M/548M [00:03<00:12, 35.8MB/s]model.safetensors:  23%|██▎       | 126M/548M [00:03<00:11, 36.0MB/s]model.safetensors:  25%|██▍       | 136M/548M [00:04<00:11, 36.0MB/s]model.safetensors:  27%|██▋       | 147M/548M [00:04<00:11, 36.1MB/s]model.safetensors:  29%|██▊       | 157M/548M [00:04<00:10, 36.2MB/s]model.safetensors:  31%|███       | 168M/548M [00:05<00:10, 36.2MB/s]model.safetensors:  33%|███▎      | 178M/548M [00:05<00:10, 36.3MB/s]model.safetensors:  34%|███▍      | 189M/548M [00:05<00:09, 36.3MB/s]model.safetensors:  36%|███▋      | 199M/548M [00:05<00:09, 36.3MB/s]model.safetensors:  38%|███▊      | 210M/548M [00:06<00:09, 36.3MB/s]model.safetensors:  40%|████      | 220M/548M [00:06<00:09, 36.3MB/s]model.safetensors:  42%|████▏     | 231M/548M [00:06<00:08, 36.4MB/s]model.safetensors:  44%|████▍     | 241M/548M [00:07<00:08, 36.4MB/s]model.safetensors:  46%|████▌     | 252M/548M [00:07<00:08, 36.4MB/s]model.safetensors:  48%|████▊     | 262M/548M [00:07<00:07, 36.4MB/s]model.safetensors:  50%|████▉     | 273M/548M [00:08<00:07, 36.3MB/s]model.safetensors:  52%|█████▏    | 283M/548M [00:08<00:07, 36.2MB/s]model.safetensors:  54%|█████▎    | 294M/548M [00:08<00:07, 36.2MB/s]model.safetensors:  55%|█████▌    | 304M/548M [00:08<00:06, 36.3MB/s]model.safetensors:  57%|█████▋    | 315M/548M [00:09<00:06, 36.3MB/s]model.safetensors:  59%|█████▉    | 325M/548M [00:09<00:06, 36.3MB/s]model.safetensors:  61%|██████    | 336M/548M [00:09<00:05, 36.3MB/s]model.safetensors:  63%|██████▎   | 346M/548M [00:10<00:05, 36.3MB/s]model.safetensors:  65%|██████▌   | 357M/548M [00:10<00:05, 35.8MB/s]model.safetensors:  67%|██████▋   | 367M/548M [00:10<00:05, 36.0MB/s]model.safetensors:  69%|██████▉   | 377M/548M [00:10<00:04, 36.1MB/s]model.safetensors:  71%|███████   | 388M/548M [00:11<00:04, 36.2MB/s]model.safetensors:  73%|███████▎  | 398M/548M [00:11<00:05, 26.9MB/s]model.safetensors:  75%|███████▍  | 409M/548M [00:12<00:04, 29.2MB/s]model.safetensors:  77%|███████▋  | 419M/548M [00:12<00:04, 31.0MB/s]model.safetensors:  78%|███████▊  | 430M/548M [00:12<00:03, 32.4MB/s]model.safetensors:  80%|████████  | 440M/548M [00:12<00:03, 33.3MB/s]model.safetensors:  82%|████████▏ | 451M/548M [00:13<00:02, 34.1MB/s]model.safetensors:  84%|████████▍ | 461M/548M [00:13<00:02, 34.8MB/s]model.safetensors:  86%|████████▌ | 472M/548M [00:13<00:02, 35.2MB/s]model.safetensors:  88%|████████▊ | 482M/548M [00:14<00:01, 35.6MB/s]model.safetensors:  90%|████████▉ | 493M/548M [00:14<00:01, 35.8MB/s]model.safetensors:  92%|█████████▏| 503M/548M [00:14<00:01, 36.0MB/s]model.safetensors:  94%|█████████▎| 514M/548M [00:15<00:00, 36.1MB/s]model.safetensors:  96%|█████████▌| 524M/548M [00:15<00:00, 35.9MB/s]model.safetensors:  98%|█████████▊| 535M/548M [00:15<00:00, 36.0MB/s]model.safetensors:  99%|█████████▉| 545M/548M [00:15<00:00, 36.1MB/s]model.safetensors: 100%|██████████| 548M/548M [00:15<00:00, 34.3MB/s]
generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 727kB/s]
current_model: gpt2, timestamp: 1700706219
  0%|          | 0/10 [00:00<?, ?it/s]/users/ob234/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 35, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 32, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 37, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 51, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 34, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
/users/ob234/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 30, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
 10%|█         | 1/10 [02:14<20:14, 134.97s/it] 20%|██        | 2/10 [04:27<17:48, 133.53s/it] 30%|███       | 3/10 [06:39<15:31, 133.06s/it] 40%|████      | 4/10 [08:52<13:17, 132.89s/it] 50%|█████     | 5/10 [11:04<11:03, 132.61s/it] 60%|██████    | 6/10 [13:17<08:50, 132.62s/it] 70%|███████   | 7/10 [15:30<06:37, 132.63s/it] 80%|████████  | 8/10 [17:42<04:25, 132.59s/it] 90%|█████████ | 9/10 [19:54<02:12, 132.46s/it]100%|██████████| 10/10 [22:06<00:00, 132.27s/it]100%|██████████| 10/10 [22:06<00:00, 132.66s/it]
end experiment :)